---
title: "Automatic model discovery using symbolic regression (I/II)"
subtitle: "ML-Club - 20.12.23"
author: "Ingo Steldermann"
fig-cap-location: margin
from: markdown+emoji
format:
  revealjs: 
    # light theme
    theme: [format.scss]
    # dark theme
    # theme: [dark, format.scss]
execute:
  eval: true
  cache: true
  freeze: true
bibliography: references.bib
---

## Style of figures
Can be changed in the `matplotlibstyle.py` file.

## Embed a figure from a notebook

<!-- {{< embed notebook.ipynb#fig-preditor-pray-data >}} -->
{{< embed sample_notebook.ipynb#fig-plot >}}

## Embed output from a notebook

::: {style="font-size: 140%;"}
{{< embed sample_notebook.ipynb#cell-output >}}
:::

## Embed existing figures

![](images/Fig3.png)

## Columns

::: columns
::: {.column width="50%"}
ODE: 
$$
\begin{align*} 
\begin{pmatrix}
\dot{w} \\ \dot{d}
\end{pmatrix}
=
\begin{pmatrix}
w(\alpha - \beta d) \\ d(-\delta + \gamma w)
\end{pmatrix}
\end{align*}
$$
:::

::: {.column width="50%"}
-   wolves: $w$, deer: $d$
-   parameters: $P=(\alpha, \beta, \gamma, \delta)$
-   initial condition: $w^0$, $d^0$
:::
:::

## Tex

```{=tex}
\begin{align*} 
\text{discrete states } & \mathbf{U} = 
\begin{bmatrix}
  \begin{pmatrix}
  w^0 \\ d^0
  \end{pmatrix},
  \begin{pmatrix}
  w^1 \\ d^1
  \end{pmatrix},
  \dots,
  \begin{pmatrix}
  w^N \\ d^N
  \end{pmatrix}
\end{bmatrix}
\in \mathbb{R}^{2(N+1)}
\\
\text{additional data }& \mathbf{Q} = 
\begin{bmatrix}
  \begin{pmatrix}
  dt^0
  \end{pmatrix},
  \begin{pmatrix}
  dt^1
  \end{pmatrix},
  \dots,
  \begin{pmatrix}
  dt^{N-1}
  \end{pmatrix}
\end{bmatrix} 
\in \mathbb{R}^{N}
\end{align*}
```
## Annotate code

The `<1>` allows you tag lines to reference later

::: {style="font-size: 200%;"}
```{python}
#| echo: true
#| eval: false
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0,1,100)    #<1>
plt.plot(x,x)               #<2>

```


1. generate data
2. plot
:::

## Compute directly in the presentation

The directives
```
#| echo: false
#| eval: true
```
enables you to commend out the input/output

```{python}
#| echo: false
#| eval: true
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0,1,100)    
plt.plot(x,x)               

```

## Declare commands / Hide content

Using 
```
::: hidden
$$
\DeclareMathOperator*{\argmin}{arg\,min}
$$
:::
```
we can define a latex operator if needed.

::: hidden
$$
\DeclareMathOperator*{\argmin}{arg\,min}
$$
:::

```{=tex}
\begin{align*}
  & \hat{\mathbf{\xi}} = \argmin_{\mathbf{\xi}} || \mathbf{U}_t - \mathbf{\Theta}(\mathbf{U}, \mathbf{Q}) \xi ||^2_2 + \epsilon \; \kappa \left( \mathbf{\Theta}(\mathbf{U}, \mathbf{Q})  \right) ||\mathbf{\xi}||_0 \\
  & \epsilon : \text{regularization parameter} \\
  & \kappa :  \text{condition number of } \mathbf{\Theta} \\
  & ||\mathbf{\xi}||_0 : \text{penalty to discourage overfitting}
\end{align*}
```

## Lists

### My suggestion:

-   SINDy: some examples (simple to complex)
    - ODE problem
    - PDE problem
    - building complex candidate libraries
    - noisy data
    - sparse data

### Are you interested in: 

- SINDy: control problem?
- PySR: explain a neural net?

## References

::: hidden
@rudy2017

@ye
:::
